# AI usage for Concurrent-data-processing-pipeline

## Overview
- Use chatGPT for better logic, example framework, example code, unit test and integration test
- Treat chatGPT as a knowledgable person but not rely on that

In my view, AI is useful in the following scenarios:
1. **Basic Structure**: To quickly scaffold the project and establish fundamental architecture.
2. **Complex Logic**: For understanding and enhancing difficult logic by providing examples or best practices.
3. **Test and Error Handling**: AI helps in generating robust testing scripts and offering ways to handle errors gracefully.

### AI Usage For Core Logic
Some of the core logic and structure are generated by AI.
Everytime I will test it and understand it totally before using that.

for CircuitBreaker
```
Prompt: Here is my present code. Tell me how to use CircuitBreaker in my code?

AI Response:
class CircuitBreaker:
    def __init__(self, failure_threshold=3, recovery_timeout=5):
        self.failure_threshold = failure_threshold
        self.failure_count = 0
        self.state = 'CLOSED'
        self.recovery_timeout = recovery_timeout
        self.last_failure_time = None

    def call(self, service_function):
        if self.state == 'OPEN':
            if time.time() - self.last_failure_time >= self.recovery_timeout:
                self.state = 'HALF-OPEN'
            else:
                raise Exception("Circuit is OPEN, service is unavailable.")
        
        try:
            result = service_function()
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise e

    def _on_success(self):
        self.failure_count = 0
        self.state = 'CLOSED'

    def _on_failure(self):
        self.failure_count += 1
        if self.failure_count >= self.failure_threshold:
            self.state = 'OPEN'
            self.last_failure_time = time.time()

async def fetch_with_retry_and_limit(client, url: str, headers: Optional[Dict[str, str]], limiter: AsyncLimiter, retries: int = 3, circuit_breaker: ``` = None):
    for attempt in range(retries):
        try:
            if circuit_breaker and circuit_breaker.state == 'OPEN':
                raise Exception("Circuit is OPEN, skipping request.")
            
            async with limiter:
                resp = await client.get(url, headers=headers)
                resp.raise_for_status()
                if circuit_breaker:
                    circuit_breaker._on_success()
                return resp
        except Exception as e:
            if circuit_breaker:
                circuit_breaker._on_failure()
            
            if attempt == retries - 1:
                raise
            backoff = 2 ** attempt + random.uniform(0, 0.5)
            await asyncio.sleep(backoff)

My Analysis: While AI suggested using the _on_success() method for success handling, I felt it was inefficient to manually check the state of the CircuitBreaker multiple times. Upon reviewing, I realized we could simply call the call() method directly to handle these checks seamlessly.

Final Implementation:
# This version improves efficiency by reducing redundant checks.
async def fetch_with_retry_and_limit(client, url: str, headers: Optional[Dict[str, str]], 
                                     limiter: AsyncLimiter, retries: int = 3, circuit_breaker: CircuitBreaker = None):
    for attempt in range(retries):
        try:
            async with limiter:
                # I call circuit_breaker to avoid using too many judgement
                resp = await circuit_breaker.call(lambda: client.get(url, headers=headers))
                resp.raise_for_status()
                return resp
        except Exception as e:
            if attempt == retries - 1:
                raise
            backoff = 2 ** attempt + random.uniform(0, 0.5)
            await asyncio.sleep(backoff)
```

for ThreadPoolExecutor
```
Prompt: Here is my present code. Based on that, how to implement adding processing pipeline and worker pools?

AI Response: 
def process_product(product: ProductItem) -> ProductItem:
    product.title = product.title.upper()
    return product

def post_process_products(products: List[ProductItem]) -> List[ProductItem]:
    with ThreadPoolExecutor(max_workers=8) as executor:
        return list(executor.map(process_product, products))

My Analysis: The AI suggested to use ThreadPoolExecutor(max_workers=8)
I modified it by adding this logic into my code. I chose this approach because it is useful and efficient.
I also add tqdm to it since it provides better visualization.

Final Implementation:
def process_files_to_products(max_workers=8) -> List[ProductItem]:
    products = []
    lock = Lock()
    filepaths = [os.path.join("temp_data", f) for f in os.listdir("temp_data")]

    total_files = len(filepaths)
    logging.info(f"Total files to process: {total_files}")

    # use ThreadPoolExecutor to process files concurrently
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # for better user experience, use tqdm to show progress
        with tqdm(total=total_files, desc="Processing files", unit="file", ncols=100) as pbar:
            for file_index, filepath in enumerate(filepaths):
                executor.submit(partial(process_file, filepath=filepath, products=products, lock=lock,
                                        file_index=file_index, total_files=total_files, pbar=pbar))

    return products
```
for process file
```
Prompt: I want to process file, I already have my local files containing my data. I have the ProductItem and the structure of process_file.
Please give me the code inside the process_file function.

AI Response: 
def process_file(filepath: str, products: List[ProductItem], lock: Lock):
    with open(filepath, "r") as f:
        raw_items = json.load(f)
        local_products = []
        for data in raw_items:
            local_products.append(ProductItem(
                source=filepath,
                id=data.get("id", ""),
                name=data.get("name", ""),
                price=data.get("price", 0.0),
            ))
    with lock:
        products.extend(local_products)

My Analysis: The AI gave me the great implementation way, but it missed fields processed_at and category.
I modified it by adding missing fields into my code.

Final Implementation:
# Process a single file and extract product items
def process_file(filepath: str, products: List[ProductItem], lock: Lock, file_index: int, total_files: int, pbar: tqdm):
    logging.debug(
        f"Started processing file {file_index + 1}/{total_files}: {filepath}")
    with open(filepath, "r") as f:
        raw_items = json.load(f)
        local_products = []
        source = os.path.basename(filepath).split("_")[0]
        for data in raw_items:
            local_products.append(ProductItem(
                id=str(data.get("id")),
                title=data.get("title") or data.get(
                    "name") or f"{data.get('first_name', '')} {data.get('last_name', '')}",
                source=source,
                price=data.get("price"),
                category=data.get("category", "unknown"),
                processed_at=datetime.now(timezone.utc).isoformat() + "Z"
            ))
    with lock:
        products.extend(local_products)
    logging.debug(
        f"Successfully added {len(local_products)} products to the list.")
    os.remove(filepath)
    logging.debug(f"File {filepath} has been removed after processing.")

    # update progress bar
    pbar.update(1)
    pbar.set_postfix(
        {"Progress": f"{(file_index + 1) / total_files * 100:.1f}% finished"})

```
### AI Usage For testing (unit test and integration test)
For unit and integration testing, I leveraged AI to generate initial test scripts and then refined them as necessary.
I hope to better test my function and do not miss a single error.

```
Prompt: Here is my function, I need to write the unit test to test its functionality.

AI Response: 
from main import fetch_with_retry_and_limit, CircuitBreaker
from aiolimiter import AsyncLimiter
import asyncio

# Test case for the fetch_with_retry_and_limit function
@pytest.mark.asyncio
async def test_fetch_with_retry_and_limit_success():
    # Mocking the httpx.AsyncClient.get method to simulate a successful response
    mock_response = AsyncMock()
    mock_response.status_code = 200
    mock_response.json.return_value = {"data": "test data"}
    
    with patch("httpx.AsyncClient.get", return_value=mock_response):
        limiter = AsyncLimiter(5, 1)  # Allow 5 requests per second
        circuit_breaker = CircuitBreaker(failure_threshold=2, recovery_timeout=5)
        
        # Call the function
        response = await fetch_with_retry_and_limit(
            client=httpx.AsyncClient(), 
            url="https://jsonplaceholder.typicode.com/posts", 
            headers={}, 
            limiter=limiter, 
            retries=3,
            circuit_breaker=circuit_breaker
        )
        
        # Assertions
        assert response.status_code == 200
        assert response.json() == {"data": "test data"}

My Analysis: The initial implementation was useful, but I encountered issues when the folder structure was not shared. 
After providing this, I updated the expected response format to match the actual data returned from the API.

Final Implementation:
from app.main import fetch_with_retry_and_limit, process_file, run_cli_aggregation, CircuitBreaker
from aiolimiter import AsyncLimiter
@pytest.mark.asyncio
async def test_fetch_with_retry_and_limit_success():
    mock_response = AsyncMock()
    mock_response.status_code = 200
    mock_response.json = AsyncMock(
        return_value={'products': [], 'total': 194, 'skip': 200, 'limit': 0})

    with patch("httpx.AsyncClient.get", return_value=mock_response):
        limiter = AsyncLimiter(5, 1)  # Allow 5 requests per second
        circuit_breaker = CircuitBreaker(
            failure_threshold=2, recovery_timeout=5)

        # Call the function
        response = await fetch_with_retry_and_limit(
            client=AsyncClient(),  # Use AsyncClient to match async behavior
            url="https://dummyjson.com/products?limit=20&skip=200",
            headers={},
            limiter=limiter,
            retries=3,
            circuit_breaker=circuit_breaker
        )

        # Assertions
        assert response.status_code == 200
        assert await response.json() == {'products': [], 'total': 194, 'skip': 200, 'limit': 0}
```

### AI Usage for error handling/logging/metrics
For this purposes, AI gave me great solutions and examples.

for logging and error handling
```
Prompt: I want to add logging for different levels for my code, please give me an example about that.

AI Response: 
import logging
# Setting up logging
logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s")

# Example of logging an error
def example_function():
    try:
        # Some code that might raise an error
        result = 1 / 0
    except Exception as e:
        logging.error(f"An error occurred: {e}")
        raise
example_function()

My Analysis: The AI's suggestion for basic logging is helpful. 
But I modified it slightly to handle different log levels and better manage log formatting for our specific needs. 
Additionally, I use it in various parts of the codebase to track progress and errors.

Final Implementation:
if __name__ == "__main__":
    args = parse_cli_args()

    endpoints = []

    if args.url:
        for entry in args.url:
            try:
                logging.info(f"Processing URL entry: {entry}")
                url, name, paged = entry.split("|")
                paged = bool(int(paged))
                endpoints.append((url, name, paged, None))
            except:
                logging.error(f"Invalid input format for --url: {entry}")
                sys.exit(1)

    if args.args_file:
        file_endpoints = read_args_from_file(args.args_file)
        logging.info(f"Processing endpoints from file: {args.args_file}")
        for entry in file_endpoints:
            url = entry.get("url")
            name = entry.get("name")
            paged = entry.get("paged", False)
            headers = entry.get("headers", None)
            if headers:
                # change headers to immutable type
                headers = frozenset(headers.items())
            endpoints.append((url, name, paged, headers))

    # deduplicate endpoints
    endpoints = list(set(endpoints))

    if endpoints:
        asyncio.run(run_cli_aggregation(endpoints, args.workers))
    else:
        logging.error("No valid URLs or file configuration found.")
        sys.exit(1)
```
for metrics and request counts
```
Prompt: I want to add metrics for my code.

AI Response: 
class PriceMetrics(BaseModel):
    min_price: Optional[float] = None
    max_price: Optional[float] = None
    average_price: Optional[float] = None
    valid_price_count: int = 0
class Metrics(BaseModel):
    price_metrics: PriceMetrics
    category_distribution: Dict[str, int] = Field(default_factory=dict)
    source_request_counts: Dict[str, int] = Field(default_factory=dict)

My Analysis: I test it and find that the output shows source_request_counts number is not correct.
I hope although it is an empty data for a URL, we still count the request number.
So I add a total_items = 0 for counting in the function.
And it worked as I expected. For https://httpbin.org/delay/1, it stills show the count = 1.

Final Implementation:
# Fetch data from paginated API and save to local files
async def fetch_paginated_to_files(endpoint_template: str, endpoint_name: str, paged: bool = False, 
                                   headers: Optional[Dict[str, str]] = None) -> Tuple[List[ErrorLog], int]:
    errors = []
    page = 1
    skip = 0
    if endpoint_name not in endpoint_limiters:
        # at most 5 requests per second
        endpoint_limiters[endpoint_name] = AsyncLimiter(5, 1)
    limiter = endpoint_limiters[endpoint_name]
    os.makedirs("temp_data", exist_ok=True)

    # this is for counting the number of requests
    total_items = 0

    circuit_breaker = CircuitBreaker(failure_threshold=3, recovery_timeout=10)
    logging.info(f"Starting to fetch data from {endpoint_name}")

    async with httpx.AsyncClient(timeout=10) as client:
        try:
            while True:
                url = endpoint_template.format(page=page, skip=skip)
                logging.debug(f"Fetching data from {url}")
                try:
                    resp = await fetch_with_retry_and_limit(client, url, headers, limiter, circuit_breaker=circuit_breaker)
                    json_data = resp.json()
                    # logging.debug(f"Received response from {url} with json_data {json_data}")
                    if isinstance(json_data, list):
                        items = json_data
                    else:
                        items = json_data.get(
                            "data") or json_data.get("products") or []

                    total_items += 1
                    if not isinstance(items, list) or not items:
                        break

                    with open(f"temp_data/{endpoint_name}_{page}_{uuid.uuid4()}.json", "w", encoding="utf-8") as f:
                        json.dump(items, f)

                    if not paged:
                        break
                    page += 1
                    skip += 20
                except Exception as e:
                    error_message = f"Error fetching data from {url}: {e}"
                    logging.error(error_message)
                    errors.append(ErrorLog(endpoint=url, error=error_message, timestamp=datetime.now(
                        timezone.utc).isoformat() + "Z"))
                    break
        except Exception as e:
            errors.append(ErrorLog(endpoint=endpoint_name, error=f"General: {e}", timestamp=datetime.now(
                timezone.utc).isoformat() + "Z"))
    return errors, total_items
```